A user tries to access www.foobar.com
. DNS resolves the www record and returns the public IP address of the load balancer instead of a single application server. The reason the load balancer is added in front is to stop everything from depending on one machine. Instead of hitting one server, the traffic is distributed between two backend servers that both run Nginx, the application code, and the database layer.

The load balancer here is HAProxy. It sits between the user and the servers, receives all incoming HTTP traffic, and sends each request to one of the two servers behind it. To keep things fairly balanced, the load balancer uses a round-robin distribution algorithm. It basically alternates: first request goes to server A, second goes to server B, third back to A, and so on. There are other algorithms, but round-robin is the simplest and works fine when the servers are identical.

With HAProxy, the setup becomes Active-Active. Both backend servers are always running and always receiving traffic. If it were Active-Passive, only one backend would be receiving requests, while the second would stay idle and only take over if the first one failed. Active-Active makes better use of resources but requires more careful handling of synchronization, especially for the database.

Because of that, the database part cannot just be two independent MySQL instances. They need to replicate. A common solution is a Primary-Replica setup (it was used to be called Master-Slave). The Primary database handles all writes — inserts, updates, deletes. The Replica gets a continuous stream of changes from the Primary and applies them so that its data stays close to current. The application servers must point write operations to the Primary. Read operations can come from either, although in this small setup both servers typically still read from the Primary to keep things simple. The Replica is mostly there as a failover or for read-heavy loads.

Even though this looks better than a single server, there are still problems. The load balancer itself is a new single point of failure. If HAProxy goes down, the entire site becomes unreachable even if the backend machines are perfectly healthy. The database Primary is another SPOF. If it dies, replication stops and write operations can no longer be handled. There are also no security layers here — no firewall, all ports open, and no HTTPS termination, which means all traffic is unencrypted. There is no monitoring either, so failures, slow queries, high CPU, or outages will not be detected until users start complaining.

So even with three servers, the infrastructure is still fragile, but it does handle more traffic than the basic one-server setup and avoids some of the limitations of having everything on a single box.